## Initial data wrangling for PhD ##
#####################################
library(readxl)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(matrixStats)
library(caret)
library(AppliedPredictiveModeling)
date()

setwd("~/GitHub/PhD")

initData<-read_excel("IGOR HUT TTRPSouthend2015.xlsx")

initData# Checking the tbl


useData<-initData[-(1:3)] # First three columns are not needed for initial analysis

glimpse(useData)

# Let's make factors out of chr markings

useData$Case<-as.factor(useData$Case)

# Check the outcome

levels(useData$Case)

# Putting all the CANCER* factor levels iside one CANCER wraper 
 
levels(useData$Case)<-sub("CANCER.*", "CANCER", levels(useData$Case))

# Check the situation

levels(useData$Case)

summary(useData$Case)


# Zeroes are a huge problem in terms of calculating mean, sd etc. so let's get rid of them
# Keeping only the columns with abs(colSum)>=0.5

# useData1<-useData[,(colSums(abs(useData[2:length(useData)])))>=0.5]

useData1<-useData[,(colSums(abs(useData[2:length(useData)])))>=5]

# Putting the first column back

useData1<-cbind(useData[1],useData1)

# Making a table again

useData1<-tbl_df(useData1)

useData1

# Puting in new varables that will serve as potential features for classification 

# as.matrix has to be used to transform df to matrix for matrixStats functions

useDataFeatures<-useData1%>%mutate(Mean=rowMeans(useData1[2:length(useData1)]), 
                                   Median=rowMedians(as.matrix(useData1[2:length(useData1)])),
                                   Sd=rowSds(as.matrix(useData1[2:length(useData1)])), 
                                   Max=rowMaxs(as.matrix(useData1[2:length(useData1)])),
                                   Min=rowMins(as.matrix(useData1[2:length(useData1)]))) 

useDataFeatures<-tbl_df(useDataFeatures)

# DF with just the newly calculated features

dataFeat<-select(useDataFeatures, Case, Mean, Median, Sd, Max, Min)

# Let's plot some data and see what can be used for features 

ggplot(dataFeat, aes(Min, Max, col=Case)) +
    geom_jitter(alpha=0.6) +
    theme_tufte()
 
ggplot(dataFeat, aes(Min, Sd, col=Case)) +
  geom_jitter(alpha=0.6) +
  theme_tufte()   

## Scatterplot Matrix 
transparentTheme(trans = .4)

featurePlot(x = dataFeat[, 2:6],
            y = dataFeat$Case,
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 7))

## Overlayed Density Plots
transparentTheme(trans = .9)
featurePlot(x = dataFeat[, 2:6],
            y = dataFeat$Case,
            plot = "density",
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(7, 1),
            auto.key = list(columns = 7))

## A bit of scaling for the "Mean" and "Median" features - trying to gain some 
## separability among groups accroding to these covariates...

dataFeat2<-mutate(dataFeat,Mean2000=Mean*2000, Median200=Median*200)

dataFeat2<-dataFeat2[-c(2:3)]

## Let's try scatter plot matrix again

transparentTheme(trans = .9)
featurePlot(x = dataFeat2[,2:6],
            y = dataFeat2$Case,
            plot = "density",
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(7, 1),
            auto.key = list(columns = 7))



                                            
# Things that ought to be done: PCA, group_by and summarise, find local maximums and minimums

dataFeat %>% group_by(Case) %>% summarise(groupAvgMax=mean(Max), groupAvgMin=mean(Min)) 

# k-means perforemed on dataFeat2 to check whether any meaningful clustering
# can be performed based on these features
set.seed(333)
clusterData<-dataFeat2[-1]
clusters<-dataFeat2$Case

kMeansClusters<-kmeans(clusterData,7) #clustring into 7 categories

table(clusters,kMeansClusters$cluster) # check does it make any sense

# Nope :(

# Let's try with data normalization and afterwards with the complete data set 
################################################################################

# We'll try RF on both useData1 and dataFeat2 to see is any classification, based on the 
# given features, feasible at all...

# Let's form the training and test sets, based on 75% and 25% of the total data (useData1),
# respectfuly

set.seed(333)

inTrain <- createDataPartition(y=useData1$Case,p=0.75, list=FALSE)

training <- useData1[inTrain,]
testing <- useData1[-inTrain,]

# Running RF

modFit <- train(Case~ ., data=training, method="rf", prox=TRUE)

# Check the model

modFit # Uzas

testPred <- predict(modFit, testing)

confusionMatrix(testPred, testing$Case) # Za plakanje

# Hajde da probamo sa dataFeat2 

# Running RF

modFit <- train(Case~., data=training, method="rf", prox=TRUE)

# Check the model

modFit # Uzas

# prediction on test data

testPred <- predict(modFit, testing)

# Results check

confusionMatrix(testPred, testing$Case) # Ocaj

# Ajd da probamo boosting, tj. boostin with trees - gbm

modFit <- train(Case~., method="gbm", data=training, verbose=FALSE)

# Check the model

print(modFit) # 

# prediction on test data

testPred <- predict(modFit, testing)

# Results check

confusionMatrix(testPred, testing$Case) # Ocaj









