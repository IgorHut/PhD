---
title: "Initial Data Wrangling and Modelling"
author: "Igor Hut"
date: '26 april 2016 '
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```


This document contains exploratory data analysis and initial tryouts of different ML algorithms for classification of the probationary data set for my PhD thesis.

For the sake of transparency and reproducibility all the used R code will be included.

## Exploratory Data Analysis

Raw data set was provided by prof. Koruga and A. Dragicevic in the form of an MS Excel file `IGOR HUT TTRPSouthend2015.xlsx`, which contains tagged results obtained by OMS recording of tissue samples. 

### Initial settings and data import

```{R, message=FALSE, warning=FALSE, cache=T}

library(readxl)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(matrixStats)
library(caret)
library(AppliedPredictiveModeling)
date()

setwd("~/GitHub/PhD")

initData<-read_excel("IGOR HUT TTRPSouthend2015.xlsx")

initData # Checking the tbl

useData<-initData[-(1:3)] # First three columns are not needed for initial analysis

useData[1:10] # Checking first columns of useData

# Let's make factors out of chr markings

useData$Case<-as.factor(useData$Case)

# Check the outcome

levels(useData$Case)

# Putting all the CANCER* factor levels iside one CANCER wraper 
 
levels(useData$Case)<-sub("CANCER.*", "CANCER", levels(useData$Case))

# Check the situation

levels(useData$Case)

summary(useData$Case)


# Zeroes are a huge problem in terms of calculating mean, sd etc. so let's get rid of them
# Keeping only the columns with abs(colSum)>=0.5

# useData1<-useData[,(colSums(abs(useData[2:length(useData)])))>=0.5]

useData1<-useData[,(colSums(abs(useData[2:length(useData)])))>=5]

# Putting the first column back

useData1<-cbind(useData[1],useData1)

# Making a table again

useData1<-tbl_df(useData1)

useData1
```

**Let's try puting in few new varables that will serve as potential features for classification:**

```{r}


# as.matrix has to be used to transform df to matrix for matrixStats functions

useDataFeatures<-useData1%>%mutate(Mean=rowMeans(useData1[2:length(useData1)]), 
                                   Median=rowMedians(as.matrix(useData1[2:length(useData1)])),
                                   Sd=rowSds(as.matrix(useData1[2:length(useData1)])), 
                                   Max=rowMaxs(as.matrix(useData1[2:length(useData1)])),
                                   Min=rowMins(as.matrix(useData1[2:length(useData1)]))) 

useDataFeatures<-tbl_df(useDataFeatures)

# DF with just the newly calculated features

dataFeat<-select(useDataFeatures, Case, Mean, Median, Sd, Max, Min)
```

** Checking whether theese features make any sense:**

```{r}
## Scatterplot Matrix 
transparentTheme(trans = .4)

featurePlot(x = dataFeat[, 2:6],
            y = dataFeat$Case,
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 7))

## Overlayed Density Plots
transparentTheme(trans = .9)
featurePlot(x = dataFeat[, 2:6],
            y = dataFeat$Case,
            plot = "density",
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(7, 1),
            auto.key = list(columns = 7))

## A bit of scaling for the "Mean" and "Median" features 

dataFeat2<-mutate(dataFeat,Mean2000=Mean*2000, Median200=Median*200)

dataFeat2<-dataFeat2[-c(2:3)]

## Let's try overlayed density plots again

transparentTheme(trans = .9)
featurePlot(x = dataFeat2[,2:6],
            y = dataFeat2$Case,
            plot = "density",
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(7, 1),
            auto.key = list(columns = 7))
```

**Let's check how `min` and `max` values behave according to groups:**

```{r}
dataFeat %>% group_by(Case) %>% summarise(groupAvgMax=mean(Max), groupAvgMin=mean(Min))

```

## ML applications

**First we will try clustering the data set into 7 clusters to check whether any meaningful clustering can be performed, based on the chosen features. Though it is obvious that these features are not adequate at all...**
  
```{r, message=FALSE, warning=FALSE}
# k-means perforemed on dataFeat2 to check whether any meaningful clustering
# can be performed based on these features
set.seed(333)
clusterData<-dataFeat2[-1]
clusters<-dataFeat2$Case

kMeansClusters<-kmeans(clusterData,7) # clustring into 7 categories

table(clusters,kMeansClusters$cluster) # check does it make any sense

# Nope :(
```

**Things to be done: clustering  with data normalization and afterwards with the complete data set, also try k-medoids and k-medians...**


### Classification with Random Forests and Stochastic Gradient Boosting

We'll try RF on both useData1 and dataFeat2 to see is any classification, based on the 
given features, feasible at all...

```{r}

# Let's form the training and test sets, based on 75% and 25% of the total data (useData1), respectfuly
set.seed(333)

inTrain <- createDataPartition(y=useData1$Case,p=0.75, list=FALSE)

training <- useData1[inTrain,]
testing <- useData1[-inTrain,]

# Running RF

modFit <- train(Case~ ., data=training, method="rf", prox=TRUE)

# Check the model

modFit # Uzas

testPred <- predict(modFit, testing)

confusionMatrix(testPred, testing$Case) # Za plakanje

# And now with dataFeat2 

# Running RF

modFit <- train(Case~., data=training, method="rf", prox=TRUE)

# Check the model

modFit # Uzas

# Prediction on test data

testPred <- predict(modFit, testing)

# Results check

confusionMatrix(testPred, testing$Case) # Ocaj

# Let's try with boosting algh.

modFit <- train(Case~., method="gbm", data=training, verbose=FALSE)

# Check the model

print(modFit) 

# Prediction on test data

testPred <- predict(modFit, testing)

# Results check

confusionMatrix(testPred, testing$Case) # Ocaj

modFit <- train(Case~., method="gbm", data=training, verbose=FALSE)

# Check the model

print(modFit) # 

# Prediction on test data

testPred <- predict(modFit, testing)

# Results check

confusionMatrix(testPred, testing$Case) 

```

**As can be seen, it is almost impossible to obtain any meaningful classification model based on the existing data.** 

**Two conditions have to be met for any further development:** 

1. *larger data set - conditio sine qua non* 

2. *defining meaningful features (covariates) that will be used for analysis and modeling - this has to be done through consultations with prof. Koruga and Sanja.* 

**Also, we can try with feature extraction directly from raw data, i.e. images.**












